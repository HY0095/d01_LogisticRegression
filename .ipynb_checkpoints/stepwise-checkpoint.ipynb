{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from IPython.display import SVG, HTML\n",
    "import copy as copy\n",
    "\n",
    "\n",
    "_model_params_doc = \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: array-like\n",
    "        The dependent variable, dim = n*1\n",
    "    x: array-like\n",
    "        The independnet variable, dim = n*p. By default, an intercept is included.\n",
    "    weight: array-like\n",
    "        Each observation in the input data set is weighted by the value of the WEIGHT variable. By default, weight is np.ones(n)\n",
    "    method: ['forward', 'backward', 'stepwise']\n",
    "        The default selection method is 'stepwise'\n",
    "    maxiter: int\n",
    "        maxiter = 25 (default)\n",
    "    mindiff: float\n",
    "        mindiff = 1e-8 (default)\n",
    "    \"\"\"\n",
    "_models_Result_docs = \"\"\"\n",
    "    params: array\n",
    "        Parameters' Estimates\n",
    "    AIC: float\n",
    "        Akaike information criterion.  `-2*(llf - p)` where `p` is the number\n",
    "        of regressors including the intercept.\n",
    "    BIC: float\n",
    "        Bayesian information criterion. `-2*llf + ln(nobs)*p` where `p` is the\n",
    "        number of regressors including the intercept.\n",
    "    SC: float\n",
    "        Schwarz criterion. `-LogL + p*(log(nobs))`\n",
    "    std_error: Array\n",
    "        The standard errors of the coefficients.(bse)\n",
    "    Chi_Square: float\n",
    "        Wald Chi-square : (logit_res.params[0]/logit_res.bse[0])**2\n",
    "    Chisqprob: float\n",
    "        P-value from Chi_square test statistic \n",
    "    llf: float\n",
    "        Value of the loglikelihood, as (LogL)\n",
    "    \"\"\"    \n",
    "# Newton-Raphson Iteration\n",
    " \n",
    "class ModelInfo(object):\n",
    "    __doc__ = \"\"\"\n",
    "    The Logistic Regression Model.\n",
    "    %(Params_doc)s\n",
    "    %(Result_doc)s\n",
    "    Notes\n",
    "    ----\n",
    "    \"\"\" % {'Params_doc' : _model_params_doc, \n",
    "           'Result_doc': _models_Result_docs}\n",
    "    \n",
    "    def __init__(self, data, role, **kwargs):\n",
    "        self.data = sm.add_constant(data)\n",
    "        self.role = role\n",
    "        self.maxiter = 25   # default maxiter\n",
    "        self.mindiff = 1e-8 # default mindiff\n",
    "        self.method  = 'None'\n",
    "        self.slentry = 0.05  # default\n",
    "        self.slstay  = 0.05  # default\n",
    "        self.cont    = True  # default self.cont = True, include intercept in model        \n",
    "        if 'slentry' in kwargs.keys():\n",
    "            self.slentry = float(kwargs['slentry'])\n",
    "        if 'slstay' in kwargs.keys():\n",
    "            self.slstay = float(kwargs['slstay'])\n",
    "        if 'method' in kwargs.keys():\n",
    "            self.method = kwargs['method']\n",
    "        if 'maxiter' in kwargs.keys():\n",
    "            self.maxiter = kwargs['maxiter']\n",
    "        if 'mindiff' in kwargs.keys():\n",
    "            self.mindiff = kwargs['mindiff'] \n",
    "    def xcol(self):\n",
    "        #print(self.role)\n",
    "        #print(self.data.columns)\n",
    "        xcols = ['const']+[self.data.columns[i+1] for i, col in enumerate(self.role) if col == 1]\n",
    "        return xcols\n",
    "    def ycol(self):\n",
    "        ycols = [self.data.columns[i+1] for i, col in enumerate(self.role) if col == 2]\n",
    "        return ycols\n",
    "    def weight(self):\n",
    "        _weight_ = [self.data.columns[i+1] for i, col in enumerate(self.role) if col == 3]\n",
    "        return _weight_\n",
    "    def handledata(self, name):\n",
    "        data = []\n",
    "        for col in name:\n",
    "            data.append(self.data[col])\n",
    "        return np.array(data)\n",
    "    def xdata(self):\n",
    "        data = self.handledata(self.xcol())\n",
    "        return data\n",
    "    def ydata(self):\n",
    "        data = self.handledata(self.ycol())\n",
    "        return data\n",
    "    def _weight(self):\n",
    "        if 3 in self.role:\n",
    "            data = self.data[self.weight()]\n",
    "        else :\n",
    "            data = np.ones(self.data.shape[0])\n",
    "        return data\n",
    "\n",
    "class LRStats(object):\n",
    "    def __init__(self, step, n, p, res):\n",
    "        self.aic  = res.aic\n",
    "        self.bic  = res.bic\n",
    "        self.logl = -2*res.llf\n",
    "        self.sc   = 2*(-res.llf + p*(np.log(n)))\n",
    "        self.params = res.params\n",
    "        self.wald_chi = (res.params/res.bse)**2 \n",
    "        self.std_error = res.bse\n",
    "        self.pchi2 = 2*stats.norm.cdf(-np.abs((res.params/res.bse)))\n",
    "    def resprint(self):\n",
    "        print(\"                          Model Fit Statistics \")\n",
    "        print(\"==============================================================================\")\n",
    "        print(\"AIC                   %s           BIC           %s    \" % (self.aic, self.bic))\n",
    "        print(\"-2Logl                %s           SC            %s    \" % (self.logl, self.sc))\n",
    "        print(\"==============================================================================\")\n",
    "\n",
    "        \n",
    "        \n",
    "class checkio(object):\n",
    "    def __init__(self,xwait,score,pvalue):\n",
    "        self.xwait = xwait\n",
    "        self.score = score\n",
    "        self.pvalue= pvalue\n",
    "    def enter(self):\n",
    "        print(\"              Analysis of Variables Eligible for Entry  \")\n",
    "        print(\"==============================================================================\")\n",
    "        print(\"\\t%5s\\t \\t%5s\\t \\t%5s\\t\" % (\"variable\", \"Wald Chi-square\", \"Pr>ChiSq\"))\n",
    "        for i,v in enumerate(self.xwait):\n",
    "            print(\"    \\t%5s\\t             \\t%10s\\t     \\t%10s\\t\" % (v, self.score[i], self.pvalue[i]))\n",
    "        print(\" \") \n",
    "    def remove(self):\n",
    "        print(\"              Analysis of Variables Eligible for Remove  \")\n",
    "        print(\"==============================================================================\")\n",
    "        print(\"\\t%5s\\t \\t%5s\\t \\t%5s\\t\" % (\"variable\", \"Wald Chi-square\", \"Pr>ChiSq\"))\n",
    "        for i,v in enumerate(self.xwait):\n",
    "            print(\"    \\t%5s\\t             \\t%10s\\t     \\t%10s\\t\" % (v, self.score[i], self.pvalue[i]))\n",
    "        print(\" \")\n",
    "        \n",
    "class GlobalNullTest(object):\n",
    "    def __init__(self,x,y,beta):\n",
    "        self.x  = x\n",
    "        self.p  = (x.shape[1] - 1)\n",
    "        self.y  = y\n",
    "        self.betai = pd.DataFrame(beta+[0.])\n",
    "    def score(self):\n",
    "        pi_value = 1/(1+np.exp(-1*np.dot(self.x, self.betai)))\n",
    "        u = np.dot( self.x.T, self.y-pi_value)\n",
    "        h = np.dot(np.dot(self.x.T, np.eye(len(self.y))*pi_value*(1-pi_value)), self.x)\n",
    "        score = np.dot(np.dot(u.T,np.linalg.inv(h)), u)\n",
    "        return list(score[0])\n",
    "    def pvalue(self):\n",
    "        pvalue = stats.chisqprob(self.score(), 1)\n",
    "        return list(pvalue)\n",
    "\n",
    "class roc(object):\n",
    "    def __init__(self,data):\n",
    "        self.data = data.sort_values('y_fit')\n",
    "        self.data['order'] = self.data.y_fit.argsort()\n",
    "        self.data.index = self.data.order.values\n",
    "    def getauc(self):\n",
    "        M = sum(self.data.y)\n",
    "        neworder  = []\n",
    "        for i, v in enumerate(self.data.y_fit):\n",
    "            if i == 0:\n",
    "                tmp   = v\n",
    "                suma  = self.data.order[i]+1.\n",
    "                t     = 0\n",
    "                count = 1\n",
    "            else:\n",
    "                if tmp != v:\n",
    "                    neworder[t:i] = np.ones(count)*(suma/count)\n",
    "                    tmp   = v\n",
    "                    t     = i\n",
    "                    count = 1\n",
    "                    suma  = self.data.order[i]+1.\n",
    "                else :\n",
    "                    suma = suma+self.data.order[i]+1.\n",
    "                    count += 1\n",
    "        if i == len(self.data)-1:\n",
    "            neworder[t:i+1] = np.ones(count)*(suma/count)\n",
    "        #print(neworder)\n",
    "        return (np.dot(self.data.y, neworder)-M*(M+1)/2)/(M*(len(self.data.y)-M))\n",
    "    \n",
    "    \n",
    "    \n",
    "class StepwiseModel(ModelInfo):\n",
    "    def __init__(self, data, role, **kwargs):\n",
    "        super(StepwiseModel, self).__init__(data, role, **kwargs)\n",
    "        super(StepwiseModel, self).xcol()\n",
    "        super(StepwiseModel, self).ydata()\n",
    "    def logitreg(self):\n",
    "        n    = self.data.shape[0]\n",
    "        p    = self.data.shape[1]\n",
    "        y    = pd.DataFrame(self.ydata()[0], columns = ['y'])\n",
    "        xcol = self.xcol()\n",
    "        #xin  = np.ones(p)\n",
    "        #xout = np.zeros(p)\n",
    "        xenter= ['const']\n",
    "        xwait = copy.copy(xcol)\n",
    "        xwait.remove('const')\n",
    "        #xout   = [] \n",
    "        step   = 0\n",
    "        history = {}\n",
    "        print(\"**** The LogitReg Process ****\\n\")\n",
    "        print(\"** Step 0. Intercept entered:\\n\")\n",
    "        logit_mod = sm.Logit(self.ydata()[0],self.xdata()[0])\n",
    "        logit_res = logit_mod.fit(disp=0)\n",
    "        Beta0     = list(logit_res.params)\n",
    "        print(logit_res.summary())\n",
    "        history = LRStats(step,n,1,logit_res)\n",
    "        print(\" \")\n",
    "        history.resprint()\n",
    "        newx   = self.data['const']\n",
    "        for i in np.arange(p):\n",
    "            print(\"   \")\n",
    "            score  = []\n",
    "            pvalue = []\n",
    "            rb     = 0\n",
    "            logit_res = {}\n",
    "            history   = {}\n",
    "            for xname in xwait:\n",
    "                _tmpx = np.vstack((newx, self.data[xname]))\n",
    "                _tmpxenter = xenter+[xname]\n",
    "                _tmpx = pd.DataFrame(_tmpx.T, columns = _tmpxenter)\n",
    "                logit_mod = sm.Logit(y,_tmpx)\n",
    "                _logit_res = logit_mod.fit(disp=0)\n",
    "                logit_res[xname] = _logit_res\n",
    "                _history = LRStats(step,n,1,_logit_res)\n",
    "                history[xname]   = _history\n",
    "                nulltest = GlobalNullTest(_tmpx, y, Beta0)\n",
    "                score  = score + list(nulltest.score())\n",
    "                pvalue = pvalue + list(nulltest.pvalue())\n",
    "                #xin += 1\n",
    "            checkenter = checkio(xwait, score, pvalue)\n",
    "            checkenter.enter()\n",
    "            if (min(pvalue) <= self.slentry):\n",
    "                # Update newx and xenter\n",
    "                xin = [xwait[ii] for ii,pv in enumerate(pvalue) if pv == min(pvalue)][0]\n",
    "                xenter = xenter+[xin]\n",
    "                newx = np.vstack((newx, self.data[xin]))\n",
    "                newx = pd.DataFrame(newx.T, columns = xenter)                \n",
    "                xwait.remove(xin)\n",
    "                step += 1\n",
    "                print(\"** step %s: %s entered:\\n\"%(step,xin))\n",
    "                print(logit_res[xin].summary())\n",
    "                Beta0     = list(logit_res[xin].params)\n",
    "                history[xin].resprint()\n",
    "                pouttest    = history[xin].pchi2[1:]\n",
    "                waldouttest = history[xin].wald_chi[1:]\n",
    "                xouttest    = xenter[1:]\n",
    "                checkout    = checkio(xouttest, waldouttest, pouttest)\n",
    "                checkout.remove()\n",
    "                while 1:\n",
    "                    if (max(pouttest) <= self.slstay):\n",
    "                        print(\"         No (additional) Variables met the %s significance level for remove into the model\"%(self.slstay))\n",
    "                        break\n",
    "                    else :\n",
    "                        _slrindex = list(pouttest).index(max(pouttest))\n",
    "                        xout = xouttest[_slrindex]\n",
    "                        step += 1\n",
    "                        print(\"step %s: %s removed:\\n\"%(step, xout))\n",
    "                        # Update newx and xenter\n",
    "                        #print(xenter)\n",
    "                        #print(newx)\n",
    "                        del newx[xout]\n",
    "                        xenter.remove(xout)\n",
    "                        #xwait.remove(xout)\n",
    "                        logit_mod = sm.Logit(y,newx)\n",
    "                        _logit_res= logit_mod.fit(disp=0)\n",
    "                        Beta0     = list(_logit_res.params)\n",
    "                        _logit_res.summary()\n",
    "                        _history  = LRStats(step,n,1,_logit_res)\n",
    "                        _history.resprint()\n",
    "                        pouttest  = _history.pchi2[1:]\n",
    "                        waldouttest= _history.wald_chi[1:]\n",
    "                        xouttest   = xenter[1:]\n",
    "                        checkout   = checkio(xouttest, waldouttest, pouttest)\n",
    "                        checkout.remove()\n",
    "                        ij = 0\n",
    "                        if (xin == xout and ij == 0):\n",
    "                            print(\"Model building terminates because the last effect entered is removed by the Wald statistic criterion\")\n",
    "                            rb = 1\n",
    "                            break\n",
    "                        else :\n",
    "                            ij += 1\n",
    "                            rb = 2\n",
    "            else :\n",
    "                print(\"    No (additional) Variables met the %s significance level for entry into the model\"%(self.slentry))\n",
    "                break\n",
    "            if rb == 1:\n",
    "                break\n",
    "            newx = newx.T\n",
    "            i += 1\n",
    "        result = {}\n",
    "        for iii, b in enumerate(Beta0):\n",
    "            result[xenter[iii]] = b\n",
    "        \n",
    "        #print(newx)\n",
    "        #print(Beta0)\n",
    "        gfun = np.dot(newx, Beta0)\n",
    "        y_fit = np.exp(gfun)/(1+np.exp(gfun))        \n",
    "        return [result, y_fit]\n",
    "    \n",
    "    #def beta(self):\n",
    "        #_beta = self.logitreg()\n",
    "        #return _beta[0]\n",
    "    \n",
    "    #def y_fit(self):\n",
    "        #y_fit = self.logitreg()\n",
    "        #return y_fit[1]\n",
    "class yfit(StepwiseModel):\n",
    "    def __init__(self, data, role, **kwargs):\n",
    "        super(yfit,self).__init__(data, role, **kwargs)\n",
    "        [beta, y_fit] = super(yfit,self).logitreg()\n",
    "        self.beta  = beta\n",
    "        self.y_fit = y_fit\n",
    "        dat = pd.DataFrame(np.vstack((self.ydata()[0], y_fit)).T, columns = ['y', 'y_fit'])\n",
    "        self.dat = dat\n",
    "        #print(self.dat)\n",
    "    def auc(self):\n",
    "        Auc = roc(self.dat)\n",
    "        return Auc.getauc()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data  = pd.read_csv('/home/dzn/Project/d05_scorecard/d01_LogisticRegression/data/x.csv', sep = ',')\n",
    "role  = [1,1,1,1,1,1,2]\n",
    "a = StepwiseModel(data, role, slentry=0.3, slstay=0.2)\n",
    "#[aaa,bbb] = a.logitreg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** The LogitReg Process ****\n",
      "\n",
      "** Step 0. Intercept entered:\n",
      "\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   27\n",
      "Model:                          Logit   Df Residuals:                       26\n",
      "Method:                           MLE   Df Model:                            0\n",
      "Date:                Thu, 21 Jan 2016   Pseudo R-squ.:                   0.000\n",
      "Time:                        17:21:17   Log-Likelihood:                -17.186\n",
      "converged:                       True   LL-Null:                       -17.186\n",
      "                                        LLR p-value:                       nan\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.6931      0.408     -1.698      0.090        -1.493     0.107\n",
      "==============================================================================\n",
      " \n",
      "                          Model Fit Statistics \n",
      "==============================================================================\n",
      "AIC                   36.3717650879           BIC           37.6676019539    \n",
      "-2Logl                34.3717650879           SC            40.9634388199    \n",
      "==============================================================================\n",
      "   \n",
      "              Analysis of Variables Eligible for Entry  \n",
      "==============================================================================\n",
      "\tvariable\t \tWald Chi-square\t \tPr>ChiSq\t\n",
      "    \t cell\t             \t1.88933755878\t     \t0.169276657378\t\n",
      "    \tsmear\t             \t1.07447985972\t     \t0.299935739546\t\n",
      "    \tinfil\t             \t1.88174858781\t     \t0.17013554236\t\n",
      "    \t   li\t             \t7.93109621143\t     \t0.00485923489668\t\n",
      "    \tblast\t             \t3.52580924286\t     \t0.060420323175\t\n",
      "    \t temp\t             \t0.659090909092\t     \t0.416881070097\t\n",
      " \n",
      "** step 1: li entered:\n",
      "\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   27\n",
      "Model:                          Logit   Df Residuals:                       25\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Thu, 21 Jan 2016   Pseudo R-squ.:                  0.2414\n",
      "Time:                        17:21:17   Log-Likelihood:                -13.036\n",
      "converged:                       True   LL-Null:                       -17.186\n",
      "                                        LLR p-value:                  0.003967\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.7771      1.379     -2.740      0.006        -6.479    -1.075\n",
      "li             2.8973      1.187      2.441      0.015         0.571     5.223\n",
      "==============================================================================\n",
      "                          Model Fit Statistics \n",
      "==============================================================================\n",
      "AIC                   30.0729645051           BIC           32.6646382371    \n",
      "-2Logl                26.0729645051           SC            32.6646382371    \n",
      "==============================================================================\n",
      "              Analysis of Variables Eligible for Remove  \n",
      "==============================================================================\n",
      "\tvariable\t \tWald Chi-square\t \tPr>ChiSq\t\n",
      "    \t   li\t             \t5.95942245748\t     \t0.0146388356821\t\n",
      " \n",
      "         No (additional) Variables met the 0.2 significance level for remove into the model\n",
      "   \n",
      "              Analysis of Variables Eligible for Entry  \n",
      "==============================================================================\n",
      "\tvariable\t \tWald Chi-square\t \tPr>ChiSq\t\n",
      "    \t cell\t             \t1.11825772504\t     \t0.290293920308\t\n",
      "    \tsmear\t             \t0.136910784466\t     \t0.7113716319\t\n",
      "    \tinfil\t             \t0.571492437321\t     \t0.449666470321\t\n",
      "    \tblast\t             \t0.0932043773376\t     \t0.760142306495\t\n",
      "    \t temp\t             \t1.25905986906\t     \t0.26182911958\t\n",
      " \n",
      "** step 2: temp entered:\n",
      "\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   27\n",
      "Model:                          Logit   Df Residuals:                       24\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Thu, 21 Jan 2016   Pseudo R-squ.:                  0.2829\n",
      "Time:                        17:21:17   Log-Likelihood:                -12.324\n",
      "converged:                       True   LL-Null:                       -17.186\n",
      "                                        LLR p-value:                  0.007735\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "const         47.8559     46.442      1.030      0.303       -43.168   138.880\n",
      "li             3.3020      1.359      2.429      0.015         0.638     5.966\n",
      "temp         -52.4331     47.493     -1.104      0.270      -145.519    40.652\n",
      "==============================================================================\n",
      "                          Model Fit Statistics \n",
      "==============================================================================\n",
      "AIC                   30.6478222686           BIC           34.5353328666    \n",
      "-2Logl                24.6478222686           SC            31.2394960006    \n",
      "==============================================================================\n",
      "              Analysis of Variables Eligible for Remove  \n",
      "==============================================================================\n",
      "\tvariable\t \tWald Chi-square\t \tPr>ChiSq\t\n",
      "    \t   li\t             \t5.90047377967\t     \t0.0151368125598\t\n",
      "    \t temp\t             \t1.21883355547\t     \t0.269589688734\t\n",
      " \n",
      "step 3: temp removed:\n",
      "\n",
      "                          Model Fit Statistics \n",
      "==============================================================================\n",
      "AIC                   30.0729645051           BIC           32.6646382371    \n",
      "-2Logl                26.0729645051           SC            32.6646382371    \n",
      "==============================================================================\n",
      "              Analysis of Variables Eligible for Remove  \n",
      "==============================================================================\n",
      "\tvariable\t \tWald Chi-square\t \tPr>ChiSq\t\n",
      "    \t   li\t             \t5.95942245748\t     \t0.0146388356821\t\n",
      " \n",
      "Model building terminates because the last effect entered is removed by the Wald statistic criterion\n",
      "***************\n",
      "    y     y_fit  order\n",
      "0   0  0.067974      0\n",
      "1   0  0.067974      1\n",
      "2   0  0.088789      2\n",
      "3   0  0.088789      3\n",
      "4   0  0.115191      4\n",
      "5   0  0.115191      5\n",
      "6   0  0.115191      6\n",
      "7   0  0.148166      7\n",
      "8   0  0.148166      8\n",
      "9   0  0.148166      9\n",
      "10  0  0.188570     10\n",
      "11  0  0.188570     11\n",
      "12  0  0.188570     12\n",
      "13  1  0.236927     13\n",
      "14  1  0.293203     14\n",
      "15  0  0.293203     15\n",
      "16  1  0.293203     16\n",
      "17  0  0.356600     17\n",
      "18  1  0.356600     18\n",
      "19  0  0.425454     19\n",
      "20  1  0.497326     20\n",
      "21  1  0.569308     21\n",
      "22  0  0.702343     22\n",
      "23  1  0.759184     23\n",
      "24  0  0.849113     24\n",
      "25  1  0.849113     25\n",
      "26  1  0.849113     26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85493827160493829"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = yfit(data, role, slentry=0.3, slstay=0.2)\n",
    "b.auc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
